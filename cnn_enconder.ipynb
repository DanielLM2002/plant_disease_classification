{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esteban Castañeda Blanco C01795\n",
    "\n",
    "Israel López Vallecillo C04396\n",
    "\n",
    "Daniel Lizano Morales C04285\n",
    "\n",
    "Ariel Solís Monge B97664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "# Importa la clase Encoder desde encoder.py\n",
    "from encoder import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_set_size = 0.8\n",
    "labeled_train_absolute_set_size = 0.1\n",
    "labeled_test_absolute_set_size = round(1 - (labeled_train_absolute_set_size + unlabeled_set_size), 2)\n",
    "\n",
    "labeled_train_relative_set_size = round((labeled_train_absolute_set_size / (1 - unlabeled_set_size)), 2)\n",
    "labeled_test_relative_set_size = 1 - labeled_train_relative_set_size\n",
    "\n",
    "\n",
    "base_dir = os.path.join('Plant_leave_diseases_dataset', 'original')\n",
    "os.makedirs('best_models', exist_ok=True)\n",
    "model_save_path = \\\n",
    "    os.path.join('best_models', f'h1_{int(unlabeled_set_size*100)}-{int(labeled_train_absolute_set_size*100)}-{int(labeled_test_absolute_set_size*100)}_classifierA_withEncoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: mps\n"
     ]
    }
   ],
   "source": [
    "# Configuración del dispositivo\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Inicializa el modelo EfficientNet con el Encoder\n",
    "encoder = Encoder().to(device)\n",
    "\n",
    "# Cargar pesos preentrenados para el encoder con map_location\n",
    "encoder.load_state_dict(torch.load('best_models/h1_80-10-10_Encoder.pth', map_location=device))\n",
    "encoder.eval()\n",
    "\n",
    "# Congelar los pesos del encoder\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "efficientnet = models.efficientnet_b2().to(device)\n",
    "\n",
    "# Modifica la primera capa convolucional de EfficientNet\n",
    "new_in_channels = 1\n",
    "original_conv1 = efficientnet.features[0][0]\n",
    "\n",
    "new_conv1 = nn.Conv2d(\n",
    "    in_channels=new_in_channels,\n",
    "    out_channels=original_conv1.out_channels,\n",
    "    kernel_size=original_conv1.kernel_size,\n",
    "    stride=original_conv1.stride,\n",
    "    padding=original_conv1.padding,\n",
    "    bias=original_conv1.bias\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de salida del encoder: torch.Size([1024, 14, 14])\n",
      "Dimensión aplanada esperada: 196\n",
      "Dimensión del output aplanado: torch.Size([1024, 196])\n",
      "Salida de la capa lineal: torch.Size([1024, 10])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    if new_in_channels == 1:\n",
    "        new_conv1.weight = nn.Parameter(original_conv1.weight.mean(dim=1, keepdim=True))\n",
    "    else:\n",
    "        new_conv1.weight[:, :3] = original_conv1.weight\n",
    "        if new_in_channels > 3:\n",
    "            for i in range(3, new_in_channels):\n",
    "                new_conv1.weight[:, i:i+1] = original_conv1.weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Reemplaza la primera capa convolucional en el modelo\n",
    "efficientnet.features[0][0] = new_conv1\n",
    "\n",
    "efficientnet.to(device)\n",
    "\n",
    "# Verificar dimensiones de salida del encoder\n",
    "dummy_input = torch.randn(32, 1, 224, 224).to(device)\n",
    "encoded_output = encoder(dummy_input)[-1]\n",
    "print(\"Dimensiones de salida del encoder:\", encoded_output.shape)\n",
    "\n",
    "# Ajustar la capa de clasificación para aceptar la salida del encoder\n",
    "flattened_dim = encoded_output.numel() // encoded_output.size(0)  # Ajustar la dimensión aplanada\n",
    "print(\"Dimensión aplanada esperada:\", flattened_dim)\n",
    "\n",
    "efficientnet.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(flattened_dim, 10).to(device),\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(efficientnet.parameters(), lr=0.001)\n",
    "\n",
    "# Verificar la dimensión aplanada y la salida de la capa lineal\n",
    "flattened_output = encoded_output.view(encoded_output.size(0), -1)\n",
    "print(\"Dimensión del output aplanado:\", flattened_output.shape)\n",
    "output = efficientnet.classifier(flattened_output)\n",
    "print(\"Salida de la capa lineal:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y preprocesa los datos\n",
    "data_transforms = {\n",
    "    'all': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "base_dir = 'Plant_leave_diseases_dataset/original'\n",
    "full_dataset = datasets.ImageFolder(base_dir, transform=data_transforms['all'])\n",
    "\n",
    "# División de datos de entrenamiento, validación y prueba\n",
    "indices = list(range(len(full_dataset)))\n",
    "image_paths = [sample[0] for sample in full_dataset.samples]\n",
    "labels = [os.path.split(os.path.dirname(path))[-1] for path in image_paths]\n",
    "\n",
    "unlabeled_set_size = 0.8\n",
    "labeled_train_absolute_set_size = 0.1\n",
    "labeled_test_absolute_set_size = round(1 - (labeled_train_absolute_set_size + unlabeled_set_size), 2)\n",
    "\n",
    "train_val_indices, _ = train_test_split(indices, test_size=unlabeled_set_size, stratify=labels, random_state=42)\n",
    "train_val_labels = [labels[i] for i in train_val_indices]\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size=labeled_test_absolute_set_size, stratify=train_val_labels, random_state=42)\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, encoder, dataloader, device, load_best_model_path=None):\n",
    "    if load_best_model_path:\n",
    "        model.load_state_dict(torch.load(load_best_model_path, map_location=device))\n",
    "        print(f\"Loaded best model from {load_best_model_path}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            encoded_output = encoder(inputs)[-1]\n",
    "            flattened_output = encoded_output.view(encoded_output.size(0), -1)\n",
    "            outputs = model.classifier(flattened_output)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "    accuracy = report['accuracy']\n",
    "    precision = {key: value['precision'] for key, value in report.items() if key not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "    recall = {key: value['recall'] for key, value in report.items() if key not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "    \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, encoder, criterion, optimizer, train_loader, val_loader, device, num_epochs=30, patience=5, save_path='best_model.pth'):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded_output = encoder(inputs)[-1]\n",
    "            flattened_output = encoded_output.view(encoded_output.size(0), -1)\n",
    "            outputs = model.classifier(flattened_output)\n",
    "            print(f'Output shape: {outputs.shape}, Labels shape: {labels.shape}')\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        val_acc, val_precision, val_recall = evaluate_model(model, encoder, val_loader, device)\n",
    "        print(f'Val Acc: {val_acc:.4f}')\n",
    "        print(f'Val Precision: {val_precision}')\n",
    "        print(f'Val Recall: {val_recall}')\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            # Guardar el mejor modelo\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'New best model saved with accuracy: {best_acc:.4f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/346 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1024, 10]), Labels shape: torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1024) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniellizano/Documents/plant_disease_classification/cnn_enconder.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniellizano/Documents/plant_disease_classification/cnn_enconder.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m train_model(efficientnet, encoder, criterion, optimizer, train_loader, val_loader, device, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, save_path\u001b[39m=\u001b[39;49mmodel_save_path)\n",
      "\u001b[1;32m/Users/daniellizano/Documents/plant_disease_classification/cnn_enconder.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniellizano/Documents/plant_disease_classification/cnn_enconder.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOutput shape: \u001b[39m\u001b[39m{\u001b[39;00moutputs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, Labels shape: \u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniellizano/Documents/plant_disease_classification/cnn_enconder.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/daniellizano/Documents/plant_disease_classification/cnn_enconder.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniellizano/Documents/plant_disease_classification/cnn_enconder.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniellizano/Documents/plant_disease_classification/cnn_enconder.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/plant_disease_classification/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/plant_disease_classification/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/plant_disease_classification/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1186\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1187\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/Documents/plant_disease_classification/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1024) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "trained_model = train_model(efficientnet, encoder, criterion, optimizer, train_loader, val_loader, device, num_epochs=50, patience=5, save_path=model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_acc, best_model_precision, best_model_recall = evaluate_model(efficientnet, encoder, val_loader, device, load_best_model_path=model_save_path)\n",
    "print(f'Best Model Accuracy: {best_model_acc:.4f}')\n",
    "print(f'Best Model Precision per class: {best_model_precision}')\n",
    "print(f'Best Model Recall per class: {best_model_recall}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
